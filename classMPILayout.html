<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.1"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Gyselalib++: MPILayout&lt; DataIdxRange, DistributedDim &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="logo.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Gyselalib++
   </div>
  </td>
   <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.svg"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.svg" alt=""/></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.1 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search','.html');
/* @license-end */
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('classMPILayout.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-types">Public Types</a> &#124;
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="classMPILayout-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">MPILayout&lt; DataIdxRange, DistributedDim &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>A class describing a way in which data may be laid out across MPI processes.  
 <a href="classMPILayout.html#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for MPILayout&lt; DataIdxRange, DistributedDim &gt;:</div>
<div class="dyncontent">
 <div class="center">
  <img src="classMPILayout.png" usemap="#MPILayout_3C_20DataIdxRange_2C_20DistributedDim_20_3E_map" alt=""/>
  <map id="MPILayout_3C_20DataIdxRange_2C_20DistributedDim_20_3E_map" name="MPILayout_3C_20DataIdxRange_2C_20DistributedDim_20_3E_map">
<area href="classIMPILayout.html" alt="IMPILayout&lt; DataIdxRange, DistributedDim... &gt;" shape="rect" coords="0,0,279,24"/>
  </map>
</div></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-types"></a>
Public Types</h2></td></tr>
<tr class="memitem:a4b457ff7c43f225f88c04bb57d8b48c5"><td class="memItemLeft" align="right" valign="top"><a id="a4b457ff7c43f225f88c04bb57d8b48c5"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPILayout.html#a4b457ff7c43f225f88c04bb57d8b48c5">idx_range_type</a> = DataIdxRange</td></tr>
<tr class="memdesc:a4b457ff7c43f225f88c04bb57d8b48c5"><td class="mdescLeft">&#160;</td><td class="mdescRight">The index range of the data. <br /></td></tr>
<tr class="separator:a4b457ff7c43f225f88c04bb57d8b48c5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5883d6c267edba25da7370986eb15720"><td class="memItemLeft" align="right" valign="top"><a id="a5883d6c267edba25da7370986eb15720"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPILayout.html#a5883d6c267edba25da7370986eb15720">distributed_sub_idx_range</a> = typename <a class="el" href="classIMPILayout.html#a880c5120b4d4e28a75c64556dea84473">base_type::distributed_sub_idx_range</a></td></tr>
<tr class="memdesc:a5883d6c267edba25da7370986eb15720"><td class="mdescLeft">&#160;</td><td class="mdescRight">The index range of the distributed section of the data. <br /></td></tr>
<tr class="separator:a5883d6c267edba25da7370986eb15720"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab4f189220fff2a19af86cfcf8132419e"><td class="memItemLeft" align="right" valign="top"><a id="ab4f189220fff2a19af86cfcf8132419e"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPILayout.html#ab4f189220fff2a19af86cfcf8132419e">distributed_type_seq</a> = typename <a class="el" href="classIMPILayout.html#a101c87361eab7bcefda79b996d8a6f24">base_type::distributed_type_seq</a></td></tr>
<tr class="memdesc:ab4f189220fff2a19af86cfcf8132419e"><td class="mdescLeft">&#160;</td><td class="mdescRight">A type sequence describing the dimensions which are distributed across MPI processes. <br /></td></tr>
<tr class="separator:ab4f189220fff2a19af86cfcf8132419e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_types_classIMPILayout"><td colspan="2" onclick="javascript:toggleInherit('pub_types_classIMPILayout')"><img src="closed.png" alt="-"/>&#160;Public Types inherited from <a class="el" href="classIMPILayout.html">IMPILayout&lt; DataIdxRange, DistributedDim... &gt;</a></td></tr>
<tr class="memitem:a266f0c2842eb56ebedf2dade7980cab9 inherit pub_types_classIMPILayout"><td class="memItemLeft" align="right" valign="top"><a id="a266f0c2842eb56ebedf2dade7980cab9"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classIMPILayout.html#a266f0c2842eb56ebedf2dade7980cab9">discrete_domain_type</a> = DataIdxRange</td></tr>
<tr class="memdesc:a266f0c2842eb56ebedf2dade7980cab9 inherit pub_types_classIMPILayout"><td class="mdescLeft">&#160;</td><td class="mdescRight">The index range of the data. <br /></td></tr>
<tr class="separator:a266f0c2842eb56ebedf2dade7980cab9 inherit pub_types_classIMPILayout"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a880c5120b4d4e28a75c64556dea84473 inherit pub_types_classIMPILayout"><td class="memItemLeft" align="right" valign="top"><a id="a880c5120b4d4e28a75c64556dea84473"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classIMPILayout.html#a880c5120b4d4e28a75c64556dea84473">distributed_sub_idx_range</a> = IdxRange&lt; DistributedDim... &gt;</td></tr>
<tr class="memdesc:a880c5120b4d4e28a75c64556dea84473 inherit pub_types_classIMPILayout"><td class="mdescLeft">&#160;</td><td class="mdescRight">The index range of the distributed section of the data. <br /></td></tr>
<tr class="separator:a880c5120b4d4e28a75c64556dea84473 inherit pub_types_classIMPILayout"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a101c87361eab7bcefda79b996d8a6f24 inherit pub_types_classIMPILayout"><td class="memItemLeft" align="right" valign="top"><a id="a101c87361eab7bcefda79b996d8a6f24"></a>
using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classIMPILayout.html#a101c87361eab7bcefda79b996d8a6f24">distributed_type_seq</a> = ddc::detail::TypeSeq&lt; DistributedDim... &gt;</td></tr>
<tr class="memdesc:a101c87361eab7bcefda79b996d8a6f24 inherit pub_types_classIMPILayout"><td class="mdescLeft">&#160;</td><td class="mdescRight">A type sequence describing the dimensions which are distributed across MPI processes. <br /></td></tr>
<tr class="separator:a101c87361eab7bcefda79b996d8a6f24 inherit pub_types_classIMPILayout"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ac4ea3f401285e400bfae8a1b963abb56"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classMPILayout.html#a4b457ff7c43f225f88c04bb57d8b48c5">idx_range_type</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classMPILayout.html#ac4ea3f401285e400bfae8a1b963abb56">distribute_idx_range</a> (<a class="el" href="classMPILayout.html#a4b457ff7c43f225f88c04bb57d8b48c5">idx_range_type</a> global_idx_range, int comm_size, int rank)</td></tr>
<tr class="memdesc:ac4ea3f401285e400bfae8a1b963abb56"><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the distributed index range which follows the chosen layout.  <a href="classMPILayout.html#ac4ea3f401285e400bfae8a1b963abb56">More...</a><br /></td></tr>
<tr class="separator:ac4ea3f401285e400bfae8a1b963abb56"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:a805e67a98b1dc70a79c9a321a0dd7adc"><td class="memTemplParams" colspan="2">template&lt;class HeadTag &gt; </td></tr>
<tr class="memitem:a805e67a98b1dc70a79c9a321a0dd7adc"><td class="memTemplItemLeft" align="right" valign="top">IdxRange&lt; HeadTag &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classMPILayout.html#a805e67a98b1dc70a79c9a321a0dd7adc">internal_distribute_idx_range</a> (IdxRange&lt; HeadTag &gt; global_idx_range, int comm_size, int rank)</td></tr>
<tr class="memdesc:a805e67a98b1dc70a79c9a321a0dd7adc"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distribute a 1D index range over the MPI processes.  <a href="classMPILayout.html#a805e67a98b1dc70a79c9a321a0dd7adc">More...</a><br /></td></tr>
<tr class="separator:a805e67a98b1dc70a79c9a321a0dd7adc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a95fcbbdda15d02011249c7d6e2848a5f"><td class="memTemplParams" colspan="2">template&lt;class HeadTag , class... Tags, std::enable_if_t&lt;(sizeof...(Tags) &gt; 0), bool &gt;  = true&gt; </td></tr>
<tr class="memitem:a95fcbbdda15d02011249c7d6e2848a5f"><td class="memTemplItemLeft" align="right" valign="top">IdxRange&lt; HeadTag, Tags... &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classMPILayout.html#a95fcbbdda15d02011249c7d6e2848a5f">internal_distribute_idx_range</a> (IdxRange&lt; HeadTag, Tags... &gt; idx_range, int comm_size, int rank)</td></tr>
<tr class="memdesc:a95fcbbdda15d02011249c7d6e2848a5f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Distribute the index range over the MPI processes.  <a href="classMPILayout.html#a95fcbbdda15d02011249c7d6e2848a5f">More...</a><br /></td></tr>
<tr class="separator:a95fcbbdda15d02011249c7d6e2848a5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="inherited"></a>
Additional Inherited Members</h2></td></tr>
<tr class="inherit_header pub_static_attribs_classIMPILayout"><td colspan="2" onclick="javascript:toggleInherit('pub_static_attribs_classIMPILayout')"><img src="closed.png" alt="-"/>&#160;Static Public Attributes inherited from <a class="el" href="classIMPILayout.html">IMPILayout&lt; DataIdxRange, DistributedDim... &gt;</a></td></tr>
<tr class="memitem:ace6ec22fc1fdc9798b7532fce56dadbf inherit pub_static_attribs_classIMPILayout"><td class="memItemLeft" align="right" valign="top"><a id="ace6ec22fc1fdc9798b7532fce56dadbf"></a>
static constexpr int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classIMPILayout.html#ace6ec22fc1fdc9798b7532fce56dadbf">n_distributed_dimensions</a></td></tr>
<tr class="memdesc:ace6ec22fc1fdc9798b7532fce56dadbf inherit pub_static_attribs_classIMPILayout"><td class="mdescLeft">&#160;</td><td class="mdescRight">The number of dimensions that are distributed across MPI processes. <br /></td></tr>
<tr class="separator:ace6ec22fc1fdc9798b7532fce56dadbf inherit pub_static_attribs_classIMPILayout"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d4e662b2ddf07d632247d503769e3b4 inherit pub_static_attribs_classIMPILayout"><td class="memItemLeft" align="right" valign="top"><a id="a8d4e662b2ddf07d632247d503769e3b4"></a>
static constexpr bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classIMPILayout.html#a8d4e662b2ddf07d632247d503769e3b4">distributed_idx_ranges_are_first</a></td></tr>
<tr class="memdesc:a8d4e662b2ddf07d632247d503769e3b4 inherit pub_static_attribs_classIMPILayout"><td class="mdescLeft">&#160;</td><td class="mdescRight">A flag to indicate whether the distributed dimensions are the dimensions which are the furthest from being contiguous in memory. <br /></td></tr>
<tr class="separator:a8d4e662b2ddf07d632247d503769e3b4 inherit pub_static_attribs_classIMPILayout"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;class DataIdxRange, class... DistributedDim&gt;<br />
class MPILayout&lt; DataIdxRange, DistributedDim &gt;</h3>

<p>A class describing a way in which data may be laid out across MPI processes. </p>
<p>This class describes the simplest way of laying out data. In this layout the data is distributed along dimensions in order until the data is distributed. It is possible that the data may not be distributed along some of the requested dimensions if such distribution is not necessary. For example if we distribute dimensions R and Theta of a (R, Theta, Phi, <a class="el" href="structVpar.html" title="Define non periodic parallel velocity .">Vpar</a>, <a class="el" href="structMu.html" title="Define non periodic magnetic momentum .">Mu</a>) grid of size 256 x 1024 x 256 x 64 x 8 over 64 processes, the theta dimension will not be distributed.</p>
<p>The data is distributed such that it is maximally distributed over each dimension (in order) such that the size of each local index range along that dimension is the same for all processes. For example if we distribute dimensions <a class="el" href="structX.html" title="A class which describes the real space in the first spatial direction X.">X</a> and <a class="el" href="structY.html" title="A class which describes the real space in the second spatial direction Y.">Y</a> of a (<a class="el" href="structX.html" title="A class which describes the real space in the first spatial direction X.">X</a>, <a class="el" href="structY.html" title="A class which describes the real space in the second spatial direction Y.">Y</a>, Z) grid of size (10, 15, 4) over 6 processes, the <a class="el" href="structX.html" title="A class which describes the real space in the first spatial direction X.">X</a> dimension will be distributed over 2 processes and the <a class="el" href="structY.html" title="A class which describes the real space in the second spatial direction Y.">Y</a> dimension will be distributed over 3 processes.</p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">DataIdxRange</td><td>The IdxRange on which the data is defined. </td></tr>
    <tr><td class="paramname">DistributedDim</td><td>The tags of the discrete dimensions which are distributed across MPI processes. </td></tr>
  </table>
  </dd>
</dl>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="ac4ea3f401285e400bfae8a1b963abb56"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac4ea3f401285e400bfae8a1b963abb56">&#9670;&nbsp;</a></span>distribute_idx_range()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class DataIdxRange , class... DistributedDim&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classMPILayout.html#a4b457ff7c43f225f88c04bb57d8b48c5">idx_range_type</a> <a class="el" href="classMPILayout.html">MPILayout</a>&lt; DataIdxRange, DistributedDim &gt;::distribute_idx_range </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classMPILayout.html#a4b457ff7c43f225f88c04bb57d8b48c5">idx_range_type</a>&#160;</td>
          <td class="paramname"><em>global_idx_range</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>comm_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rank</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Get the distributed index range which follows the chosen layout. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">global_idx_range</td><td>The global (non-distributed) index range. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comm_size</td><td>The number of MPI processes over which the data is distributed. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rank</td><td>The rank of the current MPI process.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The distributed index range. </dd></dl>

</div>
</div>
<a id="a805e67a98b1dc70a79c9a321a0dd7adc"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a805e67a98b1dc70a79c9a321a0dd7adc">&#9670;&nbsp;</a></span>internal_distribute_idx_range() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class DataIdxRange , class... DistributedDim&gt; </div>
<div class="memtemplate">
template&lt;class HeadTag &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">IdxRange&lt;HeadTag&gt; <a class="el" href="classMPILayout.html">MPILayout</a>&lt; DataIdxRange, DistributedDim &gt;::internal_distribute_idx_range </td>
          <td>(</td>
          <td class="paramtype">IdxRange&lt; HeadTag &gt;&#160;</td>
          <td class="paramname"><em>global_idx_range</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>comm_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rank</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Distribute a 1D index range over the MPI processes. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">global_idx_range</td><td>The index range to be distributed. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comm_size</td><td>The number of processes over which the data should be disctributed. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rank</td><td>The rank of the process within the current group of processes</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The distributed index range. </dd></dl>

</div>
</div>
<a id="a95fcbbdda15d02011249c7d6e2848a5f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a95fcbbdda15d02011249c7d6e2848a5f">&#9670;&nbsp;</a></span>internal_distribute_idx_range() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class DataIdxRange , class... DistributedDim&gt; </div>
<div class="memtemplate">
template&lt;class HeadTag , class... Tags, std::enable_if_t&lt;(sizeof...(Tags) &gt; 0), bool &gt;  = true&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">IdxRange&lt;HeadTag, Tags...&gt; <a class="el" href="classMPILayout.html">MPILayout</a>&lt; DataIdxRange, DistributedDim &gt;::internal_distribute_idx_range </td>
          <td>(</td>
          <td class="paramtype">IdxRange&lt; HeadTag, Tags... &gt;&#160;</td>
          <td class="paramname"><em>idx_range</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>comm_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>rank</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">protected</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Distribute the index range over the MPI processes. </p>
<p>This function is called recursively. At each pass it distributes the index range over the first dimension in the discrete index range. The remaining dimensions and processes are then handled in the recursive call.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">idx_range</td><td>The index range to be distributed. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">comm_size</td><td>The number of processes over which the data should be disctributed. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rank</td><td>The rank of the process within the current group of processes</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The distributed index range. </dd></dl>

</div>
</div>
<hr/>The documentation for this class was generated from the following file:<ul>
<li>/home/runner/work/gyselalibxx/gyselalibxx/code_branch/src/mpi_parallelisation/<a class="el" href="mpilayout_8hpp_source.html">mpilayout.hpp</a></li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="classMPILayout.html">MPILayout</a></li>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.1 </li>
  </ul>
</div>
</body>
</html>
